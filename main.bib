
@misc{2018ServerlessCommunity,
  title = {2018 {{Serverless Community Survey}}: Huge Growth in Serverless Usage},
  shorttitle = {2018 {{Serverless Community Survey}}},
  abstract = {We asked you, our dev community, how you're using serverless. And even we were surprised by how much things have grown. Ready for the data?},
  file = {/home/hcngac/Zotero/storage/26DJ8ZC4/2018-serverless-community-survey-huge-growth-usage.html},
  howpublished = {https://serverless.com/blog/2018-serverless-community-survey-huge-growth-usage},
  journal = {serverless}
}

@article{adityaWillServerlessComputing2019,
  title = {Will {{Serverless Computing Revolutionize NFV}}?},
  author = {Aditya, Paarijaat and Akkus, Istemi Ekin and Beck, Andre and Chen, Ruichuan and Hilt, Volker and Rimac, Ivica and Satzke, Klaus and Stein, Manuel},
  year = {2019},
  month = apr,
  volume = {107},
  pages = {667--678},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2019.2898101},
  abstract = {Communication networks need to be both adaptive and scalable. The last few years have seen an explosive growth of software-defined networking (SDN) and network function virtualization (NFV) to address this need. Both technologies help enable networking software to be decoupled from the hardware so that software functionality is no longer constrained by the underlying hardware and can evolve independently. Both SDN and NFV aim to advance a software-based approach to networking, where networking functionality is implemented in software modules and executed on a suitable cloud computing platform. Achieving this goal requires the virtualization paradigm used in these services that play an important role in the transition to software-based networks. Consequently, the corresponding computing platforms accompanying the virtualization technologies need to provide the required agility, robustness, and scalability for the services executed. Serverless computing has recently emerged as a new paradigm in virtualization and has already significantly changed the economics of offloading computations to the cloud. It is considered as a low-latency, resource-efficient, and rapidly deployable alternative to traditional virtualization approaches, such as those based on virtual machines and containers. Serverless computing provides scalability and cost reduction, without requiring any additional configuration overhead on the part of the developer. In this paper, we explore and survey how serverless computing technology can help building adaptive and scalable networks and show the potential pitfalls of doing so.},
  file = {/home/hcngac/Zotero/storage/USZLW7Z9/Aditya et al. - 2019 - Will Serverless Computing Revolutionize NFV.pdf;/home/hcngac/Zotero/storage/F57JHMHU/8653379.html},
  journal = {Proceedings of the IEEE},
  keywords = {Application,Application virtualization,cloud computing,Cloud computing,communication networks,Communication netwowrks,edge computing,Edge computing,Favourite,Hardware,network function virtualization,Network function virtualization,network function virtualization (NFV),networking functionality,networking software,Read,Scalability,scalable networks,SDN,serverless computing,serverless computing technology,Servers,software defined networking,software functionality,software modules,software-based approach,software-based networks,software-defined networking (SDN),underlying hardware,virtual machines,virtualisation,Virtualization,virtualization paradigm,virtualization technologies},
  number = {4}
}

@inproceedings{agacheFirecrackerLightweightVirtualization2020,
  title = {Firecracker: {{Lightweight Virtualization}} for {{Serverless Applications}}},
  shorttitle = {Firecracker},
  booktitle = {17th \{\vphantom\}{{USENIX}}\vphantom\{\} {{Symposium}} on {{Networked Systems Design}} and {{Implementation}} (\{\vphantom\}{{NSDI}}\vphantom\{\} 20)},
  author = {Agache, Alexandru and Brooker, Marc and Iordache, Alexandra and Liguori, Anthony and Neugebauer, Rolf and Piwonka, Phil and Popa, Diana-Maria},
  year = {2020},
  pages = {419--434},
  file = {/home/hcngac/Zotero/storage/BIKJWXLR/Agache et al. - 2020 - Firecracker Lightweight Virtualization for Server.pdf;/home/hcngac/Zotero/storage/4R5Y9JDA/agache.html},
  isbn = {978-1-939133-13-7},
  keywords = {Cold Start,Favourite,Infrastructure,Isolation,Read},
  language = {en}
}

@article{aghaFoundationActorComputation1997,
  title = {A Foundation for Actor Computation},
  author = {Agha, Gul A. and Mason, Ian A. and Smith, Scott F. and Talcott, Carolyn L.},
  year = {1997},
  month = jan,
  volume = {7},
  pages = {1--72},
  publisher = {{Cambridge University Press}},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S095679689700261X},
  abstract = {We present an actor language which is an extension of a simple functional language, and provide an operational semantics for this extension. Actor configurations represent open distributed systems, by which we mean that the specification of an actor system explicitly takes into account the interface with external components. We study the composability of such systems. We define and study various notions of testing equivalence on actor expressions and configurations. The model we develop provides fairness. An important result is that the three forms of equivalence, namely, convex, must, and may equivalences, collapse to two in the presence of fairness. We further develop methods for proving laws of equivalence and provide example proofs to illustrate our methodology.},
  file = {/home/hcngac/Zotero/storage/QFCYFNSW/Agha et al. - 1997 - A foundation for actor computation.pdf;/home/hcngac/Zotero/storage/2UMSNW8E/E9A5266BA5D37A1856D50C939679F31C.html},
  journal = {Journal of Functional Programming},
  language = {en},
  number = {1}
}

@article{akkusSANDHighPerformanceServerless,
  title = {{{SAND}}: {{Towards High}}-{{Performance Serverless Computing}}},
  author = {Akkus, Istemi Ekin and Chen, Ruichuan and Rimac, Ivica and Stein, Manuel and Satzke, Klaus and Beck, Andre and Aditya, Paarijaat and Hilt, Volker},
  pages = {14},
  abstract = {Serverless computing has emerged as a new cloud computing paradigm, where an application consists of individual functions that can be separately managed and executed. However, existing serverless platforms normally isolate and execute functions in separate containers, and do not exploit the interactions among functions for performance. These practices lead to high startup delays for function executions and inefficient resource usage.},
  file = {/home/hcngac/Zotero/storage/NTKUCVET/Akkus et al. - SAND Towards High-Performance Serverless Computin.pdf},
  keywords = {Communication,Favourite,Isolation},
  language = {en}
}

@article{armstrongErlang2010,
  title = {Erlang},
  author = {Armstrong, Joe},
  year = {2010},
  month = sep,
  volume = {53},
  pages = {68--75},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1810891.1810910},
  file = {/home/hcngac/Zotero/storage/VE5Y5JKT/Armstrong - 2010 - Erlang.pdf},
  journal = {Communications of the ACM},
  language = {en},
  number = {9}
}

@inproceedings{armstrongHistoryErlang2007,
  title = {A History of {{Erlang}}},
  booktitle = {Proceedings of the Third {{ACM SIGPLAN}} Conference on {{History}} of Programming Languages},
  author = {Armstrong, Joe},
  year = {2007},
  month = jun,
  pages = {6-1--6-26},
  publisher = {{Association for Computing Machinery}},
  address = {{San Diego, California}},
  doi = {10.1145/1238844.1238850},
  abstract = {Erlang was designed for writing concurrent programs that "run forever." Erlang uses concurrent processes to structure the program. These processes have no shared memory and communicate by asynchronous message passing. Erlang processes are lightweight and belong to the language, not the operating system. Erlang has mechanisms to allow programs to change code "on the fly" so that programs can evolve and change as they run. These mechanisms simplify the construction of software for implementing non-stop systems. This paper describes the history of Erlang. Material for the paper comes from a number of different sources. These include personal recollections, discussions with colleagues, old newspaper articles and scanned copies of Erlang manuals, photos and computer listings and articles posted to Usenet mailing lists.},
  file = {/home/hcngac/Zotero/storage/F4P9WTKG/Armstrong - 2007 - A history of Erlang.pdf},
  isbn = {978-1-59593-766-7},
  series = {{{HOPL III}}}
}

@misc{AWSLambdaPricing,
  title = {{{AWS Lambda}} \textendash{} {{Pricing}}},
  abstract = {With AWS Lambda, you pay only for what you use. You are charged based on the number of requests for your functions and the time your code executes.},
  file = {/home/hcngac/Zotero/storage/6WP8BFY2/pricing.html},
  howpublished = {https://aws.amazon.com/lambda/pricing/},
  journal = {Amazon Web Services, Inc.},
  language = {en-US}
}

@misc{AWSLambdaRuntimes,
  title = {{{AWS Lambda}} Runtimes - {{AWS Lambda}}},
  file = {/home/hcngac/Zotero/storage/TBJI9IA8/lambda-runtimes.html},
  howpublished = {https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html}
}

@article{bailisHighlyAvailableTransactions2013,
  title = {Highly {{Available Transactions}}: {{Virtues}} and {{Limitations}} ({{Extended Version}})},
  shorttitle = {Highly {{Available Transactions}}},
  author = {Bailis, Peter and Davidson, Aaron and Fekete, Alan and Ghodsi, Ali and Hellerstein, Joseph M. and Stoica, Ion},
  year = {2013},
  month = oct,
  abstract = {To minimize network latency and remain online during server failures and network partitions, many modern distributed data storage systems eschew transactional functionality, which provides strong semantic guarantees for groups of multiple operations over multiple data items. In this work, we consider the problem of providing Highly Available Transactions (HATs): transactional guarantees that do not suffer unavailability during system partitions or incur high network latency. We introduce a taxonomy of highly available systems and analyze existing ACID isolation and distributed data consistency guarantees to identify which can and cannot be achieved in HAT systems. This unifies the literature on weak transactional isolation, replica consistency, and highly available systems. We analytically and experimentally quantify the availability and performance benefits of HATs--often two to three orders of magnitude over wide-area networks--and discuss their necessary semantic compromises.},
  archiveprefix = {arXiv},
  eprint = {1302.0309},
  eprinttype = {arxiv},
  file = {/home/hcngac/Zotero/storage/2DUKGIHE/Bailis et al. - 2013 - Highly Available Transactions Virtues and Limitat.pdf;/home/hcngac/Zotero/storage/KVWQXEZA/1302.html},
  journal = {arXiv:1302.0309 [cs]},
  keywords = {Computer Science - Databases,Favourite,Read},
  primaryclass = {cs}
}

@techreport{bakerLawsCommunicatingParallel1977,
  title = {Laws for {{Communicating Parallel Processes}}},
  author = {Baker, Henry and Hewitt, Carl},
  year = {1977},
  month = may,
  institution = {{MIT Artificial Intelligence Laboratory}},
  abstract = {This paper presents some laws that must be satisfied by computations involving communicating parallel processes. The laws are stated in the context of the actor theory, a model for distributed parallel computation, and take the form of stating plausible restrictions on the histories of parallel computations to make them physically realizable. The laws are justified by appeal to physical intuition and are to be regarded as falsifiable assertions about the kinds of computations that occur in nature rather than as proven theorems in mathematics. The laws are used to analyze the mechanisms by which multiple processes can communicate to work effectively together to solve difficult problems.  Since the causal relations among the events in a parallel computation do not specify a total order on events, the actor model generalizes the notion of computation from a sequence of states to a partial order of events. The interpretation of unordered events in this partial order is that they proceed concurrently. The utility of partial orders is demonstrated by using them to express our laws for distributed computation.},
  annotation = {Accepted: 2008-08-26T14:05:51Z},
  file = {/home/hcngac/Zotero/storage/5VPZXFTM/Baker and Hewitt - 1977 - Laws for Communicating Parallel Processes.pdf;/home/hcngac/Zotero/storage/GIB684VB/41962.html},
  language = {en},
  type = {Working {{Paper}}}
}

@article{bernsteinDevelopingCloudServices2016a,
  ids = {bernsteinDevelopingCloudServices2016},
  title = {Developing {{Cloud Services Using}} the {{Orleans Virtual Actor Model}}},
  author = {Bernstein, Philip A. and Bykov, Sergey},
  year = {2016},
  month = sep,
  volume = {20},
  pages = {71--75},
  issn = {1941-0131},
  doi = {10.1109/MIC.2016.108},
  abstract = {Orleans provides a straightforward approach to building distributed interactive applications for the Cloud, without having to learn complex programming patterns for handling concurrency, fault tolerance, and resource management. Orleans was made available as open source in January 2015.},
  file = {/home/hcngac/Zotero/storage/HF65IU2R/Bernstein and Bykov - 2016 - Developing Cloud Services Using the Orleans Virtua.pdf;/home/hcngac/Zotero/storage/EFJ5SXIT/7676196.html},
  journal = {IEEE Internet Computing},
  keywords = {Actuators,cloud computing,Cloud computing,cloud services,concurrency handling,distributed interactive applications,Distributed processing,fault tolerance,Internet/Web technologies,Orleans,Orleans virtual actor model,resource management,Scalability,scalable distributed computing,virtual actors,virtual reality,Virtual reality,Web and internet services},
  number = {5}
}

@article{bernsteinGeodistributionActorbasedServices2017,
  title = {Geo-Distribution of Actor-Based Services},
  author = {Bernstein, Philip A. and Burckhardt, Sebastian and Bykov, Sergey and Crooks, Natacha and Faleiro, Jose M. and Kliot, Gabriel and Kumbhare, Alok and Rahman, Muntasir Raihan and Shah, Vivek and Szekeres, Adriana and Thelin, Jorgen},
  year = {2017},
  month = oct,
  volume = {1},
  pages = {107:1--107:26},
  doi = {10.1145/3133931},
  abstract = {Many service applications use actors as a programming model for the middle tier, to simplify synchronization, fault-tolerance, and scalability. However, efficient operation of such actors in multiple, geographically distant datacenters is challenging, due to the very high communication latency. Caching and replication are essential to hide latency and exploit locality; but it is not a priori clear how to combine these techniques with the actor programming model. We present Geo, an open-source geo-distributed actor system that improves performance by caching actor states in one or more datacenters, yet guarantees the existence of a single latest version by virtue of a distributed cache coherence protocol. Geo's programming model supports both volatile and persistent actors, and supports updates with a choice of linearizable and eventual consistency. Our evaluation on several workloads shows substantial performance benefits, and confirms the advantage of supporting both replicated and single-instance coherence protocols as configuration choices. For example, replication can provide fast, always-available reads and updates globally, while batching of linearizable storage accesses at a single location can boost the throughput of an order processing workload by 7x.},
  file = {/home/hcngac/Zotero/storage/M3C2XFNE/Bernstein et al. - 2017 - Geo-distribution of actor-based services.pdf},
  journal = {Proceedings of the ACM on Programming Languages},
  keywords = {Cloud Services,Consistency,Geo-Distribution,Virtual Actors},
  number = {OOPSLA}
}

@article{bernsteinIndexingActorOrientedDatabase,
  title = {Indexing in an {{Actor}}-{{Oriented Database}}},
  author = {Bernstein, Philip A and Dashti, Mohammad and Kiefer, Tim and Maier, David},
  pages = {11},
  abstract = {Many of today's interactive server applications are implemented using actor-oriented programming frameworks. Such applications treat actors as a distributed in-memory object-oriented database. However, actor programming frameworks offer few if any database system features, leaving application developers to fend for themselves. It is challenging to add such features because the design space is different than traditional database systems. The system must be scalable to a large number of servers, it must work well with a variety of cloud storage services, and it must integrate smoothly with the actor programming model.},
  file = {/home/hcngac/Zotero/storage/GBTKVWG5/Bernstein et al. - Indexing in an Actor-Oriented Database.pdf},
  language = {en}
}

@article{bernsteinOrleansDistributedVirtual2014,
  title = {Orleans: {{Distributed Virtual Actors}} for {{Programmability}} and {{Scalability}}},
  shorttitle = {Orleans},
  author = {Bernstein, Phil and Bykov, Sergey and Geller, Alan and Kliot, Gabriel and Thelin, Jorgen},
  year = {2014},
  month = mar,
  abstract = {High-scale interactive services demand high throughput with low latency and high availability, difficult goals to meet with the traditional stateless 3-tier architecture. The actor model makes it natural to build a stateful middle tier and achieve the required performance. However, the popular actor model platforms still pass many distributed systems problems to the developers. The \ldots},
  file = {/home/hcngac/Zotero/storage/KQVZXQYL/Bernstein et al. - 2014 - Orleans Distributed Virtual Actors for Programmab.pdf;/home/hcngac/Zotero/storage/85AFV4AU/orleans-distributed-virtual-actors-for-programmability-and-scalability.html},
  language = {en-US}
}

@inproceedings{bykovOrleansCloudComputing2011,
  title = {Orleans: Cloud Computing for Everyone},
  shorttitle = {Orleans},
  booktitle = {Proceedings of the 2nd {{ACM Symposium}} on {{Cloud Computing}}},
  author = {Bykov, Sergey and Geller, Alan and Kliot, Gabriel and Larus, James R. and Pandya, Ravi and Thelin, Jorgen},
  year = {2011},
  month = oct,
  pages = {1--14},
  publisher = {{Association for Computing Machinery}},
  address = {{Cascais, Portugal}},
  doi = {10.1145/2038916.2038932},
  abstract = {Cloud computing is a new computing paradigm, combining diverse client devices -- PCs, smartphones, sensors, single-function, and embedded -- with computation and data storage in the cloud. As with every advance in computing, programming is a fundamental challenge, as the cloud is a concurrent, distributed system running on unreliable hardware and networks. Orleans is a software framework for building reliable, scalable, and elastic cloud applications. Its programming model encourages the use of simple concurrency patterns that are easy to understand and employ correctly. It is based on distributed actor-like components called grains, which are isolated units of state and computation that communicate through asynchronous messages. Within a grain, promises are the mechanism for managing both asynchronous messages and local task-based concurrency. Isolated state and a constrained execution model allow Orleans to persist, migrate, replicate, and reconcile grain state. In addition, Orleans provides lightweight transactions that support a consistent view of state and provide a foundation for automatic error handling and failure recovery. We implemented several applications in Orleans, varying from a messaging-intensive social networking application to a data- and compute-intensive linear algebra computation. The programming model is a general one, as Orleans allows the communications to evolve dynamically at runtime. Orleans enables a developer to concentrate on application logic, while the Orleans runtime provides scalability, availability, and reliability.},
  file = {/home/hcngac/Zotero/storage/U9BSRWB7/Bykov et al. - 2011 - Orleans cloud computing for everyone.pdf},
  isbn = {978-1-4503-0976-9},
  keywords = {cloud computing,distributed actors,Favourite,programming models,Read},
  series = {{{SOCC}} '11}
}

@inproceedings{caddenSEUSSSkipRedundant2020,
  title = {{{SEUSS}}: Skip Redundant Paths to Make Serverless Fast},
  shorttitle = {{{SEUSS}}},
  booktitle = {Proceedings of the {{Fifteenth European Conference}} on {{Computer Systems}}},
  author = {Cadden, James and Unger, Thomas and Awad, Yara and Dong, Han and Krieger, Orran and Appavoo, Jonathan},
  year = {2020},
  month = apr,
  pages = {1--15},
  publisher = {{Association for Computing Machinery}},
  address = {{Heraklion, Greece}},
  doi = {10.1145/3342195.3392698},
  abstract = {This paper presents a system-level method for achieving the rapid deployment and high-density caching of serverless functions in a FaaS environment. For reduced start times, functions are deployed from unikernel snapshots, bypassing expensive initialization steps. To reduce the memory footprint of snapshots we apply page-level sharing across the entire software stack that is required to run a function. We demonstrate the effects of our techniques by replacing Linux on the compute node of a FaaS platform architecture. With our prototype OS, the deployment time of a function drops from 100s of milliseconds to under 10 ms. Platform throughput improves by 51x on workload composed entirely of new functions. We are able to cache over 50,000 function instances in memory as opposed to 3,000 using standard OS techniques. In combination, these improvements give the FaaS platform a new ability to handle large-scale bursts of requests.},
  file = {/home/hcngac/Zotero/storage/AUQB6S4P/Cadden et al. - 2020 - SEUSS skip redundant paths to make serverless fas.pdf},
  isbn = {978-1-4503-6882-7},
  keywords = {Cold Start,Favourite,Read},
  series = {{{EuroSys}} '20}
}

@inproceedings{carreiraCirrusServerlessFramework2019,
  title = {Cirrus: A {{Serverless Framework}} for {{End}}-to-End {{ML Workflows}}},
  shorttitle = {Cirrus},
  booktitle = {Proceedings of the {{ACM Symposium}} on {{Cloud Computing}}},
  author = {Carreira, Joao and Fonseca, Pedro and Tumanov, Alexey and Zhang, Andrew and Katz, Randy},
  year = {2019},
  month = nov,
  pages = {13--24},
  publisher = {{Association for Computing Machinery}},
  address = {{Santa Cruz, CA, USA}},
  doi = {10.1145/3357223.3362711},
  abstract = {Machine learning (ML) workflows are extremely complex. The typical workflow consists of distinct stages of user interaction, such as preprocessing, training, and tuning, that are repeatedly executed by users but have heterogeneous computational requirements. This complexity makes it challenging for ML users to correctly provision and manage resources and, in practice, constitutes a significant burden that frequently causes over-provisioning and impairs user productivity. Serverless computing is a compelling model to address the resource management problem, in general, but there are numerous challenges to adopt it for existing ML frameworks due to significant restrictions on local resources. This work proposes Cirrus---an ML framework that automates the end-to-end management of datacenter resources for ML workflows by efficiently taking advantage of serverless infrastructures. Cirrus combines the simplicity of the serverless interface and the scalability of the serverless infrastructure (AWS Lambdas and S3) to minimize user effort. We show a design specialized for both serverless computation and iterative ML training is needed for robust and efficient ML training on serverless infrastructure. Our evaluation shows that Cirrus outperforms frameworks specialized along a single dimension: Cirrus is 100x faster than a general purpose serverless system [36] and 3.75x faster than specialized ML frameworks for traditional infrastructures [49].},
  file = {/home/hcngac/Zotero/storage/KTMMTPMA/Carreira et al. - 2019 - Cirrus a Serverless Framework for End-to-end ML W.pdf},
  isbn = {978-1-4503-6973-2},
  keywords = {Distributed Computing,Machine Learning,Serverless},
  series = {{{SoCC}} '19}
}

@inproceedings{chuangEventWaveProgrammingModel2013,
  title = {{{EventWave}}: Programming Model and Runtime Support for Tightly-Coupled Elastic Cloud Applications},
  shorttitle = {{{EventWave}}},
  booktitle = {Proceedings of the 4th Annual {{Symposium}} on {{Cloud Computing}}},
  author = {Chuang, Wei-Chiu and Sang, Bo and Yoo, Sunghwan and Gu, Rui and Kulkarni, Milind and Killian, Charles},
  year = {2013},
  month = oct,
  pages = {1--16},
  publisher = {{Association for Computing Machinery}},
  address = {{Santa Clara, California}},
  doi = {10.1145/2523616.2523617},
  abstract = {An attractive approach to leveraging the ability of cloud-computing platforms to provide resources on demand is to build elastic applications, which can dynamically scale up or down based on resource requirements. To ease the development of elastic applications, it is useful for programmers to write applications with simple sequential semantics, without considering elasticity, and rely on runtime support to provide that elasticity. While this approach has been useful in restricted domains, such as MapReduce, existing programming models for general distributed applications do not expose enough information about their inherent organization of state and computation to provide such transparent elasticity. We introduce EventWave, an event-driven programming model that allows developers to design elastic programs with inelastic semantics while naturally exposing isolated state and computation with programmatic parallelism. In addition, we describe the runtime mechanism which takes the exposed parallelism to provide elasticity. Finally, we evaluate our implementation through microbenchmarks and case studies to demonstrate that EventWave can provide efficient, scalable, transparent elasticity for applications run in the cloud.},
  file = {/home/hcngac/Zotero/storage/YZHNYCLK/Chuang et al. - 2013 - EventWave programming model and runtime support f.pdf},
  isbn = {978-1-4503-2428-1},
  series = {{{SOCC}} '13}
}

@article{cicconettiDistributedComputingEnvironments2020,
  title = {Toward {{Distributed Computing Environments}} with {{Serverless Solutions}} in {{Edge Systems}}},
  author = {Cicconetti, Claudio and Conti, Marco and Passarella, Andrea and Sabella, Dario},
  year = {2020},
  month = mar,
  volume = {58},
  pages = {40--46},
  issn = {1558-1896},
  doi = {10.1109/MCOM.001.1900498},
  abstract = {Computation offloading through stateless applications is gaining momentum thanks to the emergence of serverless frameworks with inherent scalability properties. However, adoption of a serverless framework in an edge computing system requires careful consideration to keep its advantages unscathed. In the cloud, micro-services are scaled automatically according to demands, but in edge computing this would incur a significantly higher cost than in a data center and cannot be as fluid. This is especially relevant in scenarios where edge nodes are spread across large areas and have relatively small computation capabilities. In this article we propose to overcome this issue by adapting the allocation of demands to the currently allocated micro-services at short timescales, with two alternative mechanisms designed for different target scenarios, both aimed at enabling distributed computing environments. The proposed solution can be integrated within the ETSI MEC standard, which specifies a reference architecture and open service interfaces. Our contribution is validated in a proof-of-concept scenario with a prototype implementation released as open source.},
  file = {/home/hcngac/Zotero/storage/24572I2X/Cicconetti et al. - 2020 - Toward Distributed Computing Environments with Ser.pdf;/home/hcngac/Zotero/storage/Y7Y9XAKQ/9040261.html},
  journal = {IEEE Communications Magazine},
  keywords = {cloud computing,Cloud computing,computation offloading,Computer architecture,data center,Data centers,distributed computing environments,Distributing computing,Edge computing,edge computing system,edge nodes,ETSI MEC standard,FAA,Favourite,open service interfaces,Read,reference architecture,Resource management,scalability properties,serverless framework,serverless solutions,Servers,Standards,stateless applications,Web services},
  number = {3}
}

@misc{CirrusProceedingsACM,
  title = {Cirrus | {{Proceedings}} of the {{ACM Symposium}} on {{Cloud Computing}}},
  file = {/home/hcngac/Zotero/storage/5XTQRH88/3357223.html},
  howpublished = {https://dl-acm-org.lib.ezproxy.ust.hk/doi/abs/10.1145/3357223.3362711?casa\_token=fM\_gI58DbCEAAAAA:Fv66m\_JtSv5WrEGS\_bEs6a016czf4O0g8\_V1HiKd\_3ay0j-yHCdVbVTSfZkDIKQft4MQwQWZw3U}
}

@article{conwayLogicLatticesDistributed,
  title = {Logic and Lattices for Distributed Programming},
  author = {Conway, Neil and Marczak, William R and Alvaro, Peter and Hellerstein, Joseph M and Maier, David},
  pages = {14},
  abstract = {In recent years there has been interest in achieving application-level consistency criteria without the latency and availability costs of strongly consistent storage infrastructure. A standard technique is to adopt a vocabulary of commutative operations; this avoids the risk of inconsistency due to message reordering. Another approach was recently captured by the CALM theorem, which proves that logically monotonic programs are guaranteed to be eventually consistent. In logic languages such as Bloom, CALM analysis can automatically verify that programs achieve consistency without coordination.},
  file = {/home/hcngac/Zotero/storage/P62Q57V3/Conway et al. - Logic and lattices for distributed programming.pdf},
  keywords = {Favourite,Read},
  language = {en}
}

@article{deanMapReduceSimplifiedData2008,
  title = {{{MapReduce}}: Simplified Data Processing on Large Clusters},
  shorttitle = {{{MapReduce}}},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  year = {2008},
  month = jan,
  volume = {51},
  pages = {107--113},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1327452.1327492},
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper.},
  file = {/home/hcngac/Zotero/storage/2L24WB8Q/Dean and Ghemawat - 2008 - MapReduce simplified data processing on large clu.pdf},
  journal = {Communications of the ACM},
  language = {en},
  number = {1}
}

@misc{EC2InstancePricing,
  title = {{{EC2 Instance Pricing}} \textendash{} {{Amazon Web Services}} ({{AWS}})},
  abstract = {Amazon EC2 pricing is based on instance types and the region in which your instances are running. There is no minimum fee and you only pay for what you use.},
  file = {/home/hcngac/Zotero/storage/S64RIMR2/on-demand.html},
  howpublished = {https://aws.amazon.com/ec2/pricing/on-demand/},
  journal = {Amazon Web Services, Inc.},
  language = {en-US}
}

@inproceedings{foongStorageFastRest2016,
  title = {Storage {{As Fast As Rest}} of the {{System}}},
  booktitle = {2016 {{IEEE}} 8th {{International Memory Workshop}} ({{IMW}})},
  author = {Foong, A. and Hady, F.},
  year = {2016},
  month = may,
  pages = {1--4},
  doi = {10.1109/IMW.2016.7495289},
  abstract = {Ultra-low latency, high endurance SSDs are poised to enter the market, based on 3D XPoint\texttrademark{} memory. Here we show that for these new SSDs and modern platforms, storage latency is equally divided between the SSD and the rest-of platform. We summarize some of the recent system level optimizations that make this possible. Such low latency storage offers a potential for applications to use storage as a resource in place of memory. We describe a few examples of use case analyses that we have undertaken. Finally we comment on use of 3D XPoint memory accessed as system memory rather than storage.},
  file = {/home/hcngac/Zotero/storage/84SFSWNA/Foong and Hady - 2016 - Storage As Fast As Rest of the System.pdf;/home/hcngac/Zotero/storage/ZWLU2QE9/7495289.html},
  keywords = {3D XPoint memory,disc storage,Linux,Media,optimisation,Optimization,Random access memory,random-access storage,Related,solid state disk,SSD,storage latency,Synthetic aperture sonar,system level optimization,system memory,Three-dimensional displays,Time measurement}
}

@article{fouladiEncodingFastSlowa,
  ids = {fouladiEncodingFastSlow},
  title = {Encoding, {{Fast}} and {{Slow}}: {{Low}}-{{Latency Video Processing Using Thousands}} of {{Tiny Threads}}},
  author = {Fouladi, Sadjad and Wahby, Riad S and Shacklett, Brennan and Balasubramaniam, Karthikeyan Vasuki and Zeng, William and Bhalerao, Rahul and Sivaraman, Anirudh and Porter, George and Winstein, Keith},
  pages = {15},
  abstract = {We describe ExCamera, a system that can edit, transform, and encode a video, including 4K and VR material, with low latency. The system makes two major contributions. First, we designed a framework to run general-purpose parallel computations on a commercial ``cloud function'' service. The system starts up thousands of threads in seconds and manages inter-thread communication. Second, we implemented a video encoder intended for fine-grained parallelism, using a functional-programming style that allows computation to be split into thousands of tiny tasks without harming compression efficiency. Our design reflects a key insight: the work of video encoding can be divided into fast and slow parts, with the ``slow'' work done in parallel, and only ``fast'' work done serially.},
  file = {/home/hcngac/Zotero/storage/CSFVZEF2/Fouladi et al. - Encoding, Fast and Slow Low-Latency Video Process.pdf;/home/hcngac/Zotero/storage/LAGC7X5J/Fouladi et al. - Encoding, Fast and Slow Low-Latency Video Process.pdf},
  language = {en}
}

@inproceedings{gadepalliChallengesOpportunitiesEfficient2019,
  title = {Challenges and {{Opportunities}} for {{Efficient Serverless Computing}} at the {{Edge}}},
  booktitle = {2019 38th {{Symposium}} on {{Reliable Distributed Systems}} ({{SRDS}})},
  author = {Gadepalli, Phani Kishore and Peach, Gregor and Cherkasova, Ludmila and Aitken, Rob and Parmer, Gabriel},
  year = {2019},
  month = oct,
  pages = {261--2615},
  issn = {2575-8462},
  doi = {10.1109/SRDS47363.2019.00036},
  abstract = {Serverless computing frameworks allow users to execute a small application (dedicated to a specific task) without handling operational issues such as server provisioning, resource management, and resource scaling for the increased load. Serverless computing originally emerged as a Cloud computing framework, but might be a perfect match for IoT data processing at the Edge. However, the existing serverless solutions, based on VMs and containers, are too heavy-weight (large memory footprint and high function invocation time) for operating efficiency and elastic scaling at the Edge. Moreover, many novel IoT applications require low-latency data processing and near real-time responses, which makes the current cloud-based serverless solutions unsuitable. Recently, WebAssembly (Wasm) has been proposed as an alternative method for running serverless applications at near-native speeds, while having a small memory footprint and optimized invocation time. In this paper, we discuss some existing serverless solutions, their design details, and unresolved performance challenges for an efficient serverless management at the Edge. We outline our serverless framework, called aWsm, based on the WebAssembly approach, and discuss the opportunities enabled by the aWsm design, including function profiling and SLO-driven performance management of users' functions. Finally, we present an initial assessment of aWsm performance featuring average startup time (12{$\mu$}s to 30{$\mu$}s) and an economical memory footprint (ranging from 10s to 100s of kB) for a subset of MiBench microbenchmarks used as functions.},
  file = {/home/hcngac/Zotero/storage/6DF95J34/Gadepalli et al. - 2019 - Challenges and Opportunities for Efficient Serverl.pdf;/home/hcngac/Zotero/storage/IZUM2ZVE/9049531.html},
  keywords = {average startup time,aWsm design,cloud computing,cloud computing framework,Cloud computing; Serverless; FaaS; IoT; Edge computing; WebAssembly; Wasm; performance management; SLOs.,cloud-based serverless solutions,economical memory footprint,elastic scaling,Favourite,Internet of Things,IoT applications,IoT data processing,optimized invocation time,Read,resource management,resource scaling,server provisioning,serverless applications,serverless computing frameworks,serverless framework,serverless management,SLO-driven performance management,virtual machines,Wasm,WebAssembly}
}

@misc{ggailey777SupportedLanguagesAzure,
  title = {Supported Languages in {{Azure Functions}}},
  author = {{ggailey777}},
  abstract = {Learn which languages are supported (GA) and which are in preview, and ways to extend Functions development to other languages.},
  file = {/home/hcngac/Zotero/storage/SLY6XPNK/supported-languages.html},
  howpublished = {https://docs.microsoft.com/en-us/azure/azure-functions/supported-languages},
  language = {en-us}
}

@article{hellersteinServerlessComputingOne2018,
  title = {Serverless {{Computing}}: {{One Step Forward}}, {{Two Steps Back}}},
  shorttitle = {Serverless {{Computing}}},
  author = {Hellerstein, Joseph M. and Faleiro, Jose and Gonzalez, Joseph E. and {Schleier-Smith}, Johann and Sreekanti, Vikram and Tumanov, Alexey and Wu, Chenggang},
  year = {2018},
  month = dec,
  abstract = {Serverless computing offers the potential to program the cloud in an autoscaling, pay-as-you go manner. In this paper we address critical gaps in first-generation serverless computing, which place its autoscaling potential at odds with dominant trends in modern computing: notably data-centric and distributed computing, but also open source and custom hardware. Put together, these gaps make current serverless offerings a bad fit for cloud innovation and particularly bad for data systems innovation. In addition to pinpointing some of the main shortfalls of current serverless architectures, we raise a set of challenges we believe must be met to unlock the radical potential that the cloud---with its exabytes of storage and millions of cores---should offer to innovative developers.},
  archiveprefix = {arXiv},
  eprint = {1812.03651},
  eprinttype = {arxiv},
  file = {/home/hcngac/Zotero/storage/M46P623E/Hellerstein et al. - 2018 - Serverless Computing One Step Forward, Two Steps .pdf;/home/hcngac/Zotero/storage/U6PBFUII/1812.html},
  journal = {arXiv:1812.03651 [cs]},
  keywords = {Computer Science - Databases,Computer Science - Distributed; Parallel; and Cluster Computing,Favourite},
  primaryclass = {cs}
}

@inproceedings{heTypecastingActorsAkka2014,
  title = {Typecasting Actors: From {{Akka}} to {{TAkka}}},
  shorttitle = {Typecasting Actors},
  booktitle = {Proceedings of the {{Fifth Anuual Scala Workshop}} on - {{SCALA}} '14},
  author = {He, Jiansen and Wadler, Philip and Trinder, Philip},
  year = {2014},
  pages = {23--33},
  publisher = {{ACM Press}},
  address = {{Uppsala, Sweden}},
  doi = {10.1145/2637647.2637651},
  abstract = {Scala supports actors and message passing with the Akka library. Though Scala is statically typed, messages in Akka are dynamically typed (that is, of type Any). The Akka designers argue that using static types is ``impossible'' because ``actor behaviour is dynamic'', and, indeed, it is not clear that important actor support, such as supervision or name servers, can be implemented if messages are statically typed. Here we present TAkka, a variant of Akka where messages are statically typed, and show that it is possible to implement supervisors and name servers in such a framework. We show it is possible to smoothly migrate from Akka to TAkka, porting one module at a time. We show that TAkka can support behavioural upgrades where the new message type of an actor is a supertype of the old type. We demonstrate the expressiveness of TAkka by rewriting approximately 20 Akka applications; the percentage of lines that need to be changed varies from 44\% (in a 25-line program) to 0.05\% (in a 27,000-line program), with a geometric mean of 8.5\%. We show that the execution speed, scalability, and throughput of TAkka programs are comparable to those of Akka programs.},
  file = {/home/hcngac/Zotero/storage/T4XEFD55/He et al. - 2014 - Typecasting actors from Akka to TAkka.pdf},
  isbn = {978-1-4503-2868-5},
  language = {en}
}

@article{hewittActorModelComputation2010,
  title = {Actor {{Model}} of {{Computation}}},
  author = {Hewitt, Carl},
  year = {2010},
  pages = {25},
  file = {/home/hcngac/Zotero/storage/ZCHV488Q/Hewitt - 2010 - Actor Model of Computation.pdf},
  language = {en}
}

@article{hewittActorModelComputation2015,
  title = {Actor {{Model}} of {{Computation}}: {{Scalable Robust Information Systems}}},
  shorttitle = {Actor {{Model}} of {{Computation}}},
  author = {Hewitt, Carl},
  year = {2015},
  month = jan,
  abstract = {The Actor model is a mathematical theory that treats "Actors" as the universal primitives of concurrent digital computation. The model has been used both as a framework for a theoretical understanding of concurrency, and as the theoretical basis for several practical implementations of concurrent systems. Unlike previous models of computation, the Actor model was inspired by physical laws. It was also influenced by the programming languages Lisp, Simula 67 and Smalltalk-72, as well as ideas for Petri Nets, capability-based systems and packet switching. The advent of massive concurrency through client-cloud computing and many-core computer architectures has galvanized interest in the Actor model. Actor technology will see significant application for integrating all kinds of digital information for individuals, groups, and organizations so their information usefully links together. Information integration needs to make use of the following information system principles: * Persistence. Information is collected and indexed. * Concurrency: Work proceeds interactively and concurrently, overlapping in time. * Quasi-commutativity: Information can be used regardless of whether it initiates new work or become relevant to ongoing work. * Sponsorship: Sponsors provide resources for computation, i.e., processing, storage, and communications. * Pluralism: Information is heterogeneous, overlapping and often inconsistent. * Provenance: The provenance of information is carefully tracked and recorded The Actor Model is intended to provide a foundation for inconsistency robust information integration},
  archiveprefix = {arXiv},
  eprint = {1008.1459},
  eprinttype = {arxiv},
  file = {/home/hcngac/Zotero/storage/4LJBRZ82/Hewitt - 2015 - Actor Model of Computation Scalable Robust Inform.pdf;/home/hcngac/Zotero/storage/B4Z9PSSY/1008.html},
  journal = {arXiv:1008.1459 [cs]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Programming Languages},
  primaryclass = {cs}
}

@article{hewittActorScriptTMExtension2015,
  title = {{{ActorScript}}({{TM}}) Extension of {{C}} Sharp ({{TM}}), {{Java}}({{TM}}), and {{Objective C}}({{TM}}): {{iAdaptive}}({{TM}}) Concurrency for {{antiCloud}}({{TM}}) Privacy and Security},
  shorttitle = {{{ActorScript}}({{TM}}) Extension of {{C}} Sharp ({{TM}}), {{Java}}({{TM}}), and {{Objective C}}({{TM}})},
  author = {Hewitt, Carl},
  year = {2015},
  month = mar,
  abstract = {ActorScript(TM) is a general purpose programming language for implementing discretionary, adaptive concurrency that manages resources and demand. It is differentiated from previous languages by the following: - Universality *** Ability to specify what Actors can do *** Specify interface between hardware and software *** Everything in the language is accomplished using message passing including the very definition of ActorScript itself *** Functional, Imperative, Logic, and Concurrent programming are integrated. *** Concurrency dynamically adapts to resources available and current load. *** Programs do not expose low-level implementation mechanisms such as threads, tasks, locks, cores, etc. *** Messages can be directly communicated without requiring indirection through brokers, channels, class hierarchies, mailboxes, pipes, ports, queues etc. *** Variable races are eliminated. *** Binary XML and JSON are data types. *** Application binary interfaces are afforded so that no identifier symbol need be looked up at runtime. - Safety and Security *** Programs are extension invariant, i.e., extending a program does not change its meaning. *** Applications cannot directly harm each other. - Performance *** Impose no overhead on implementation of Actor systems *** Message passing has essentially same overhead as procedure calling and looping. *** Allow execution to be dynamically adjusted for system load and capacity (e.g. cores) *** Locality because execution is not bound by a sequential global memory model *** Inherent concurrency because execution is not bound by communicating sequential processes *** Minimize latency along critical paths},
  archiveprefix = {arXiv},
  eprint = {1008.2748},
  eprinttype = {arxiv},
  file = {/home/hcngac/Zotero/storage/YUCJCMFH/Hewitt - 2015 - ActorScript(TM) extension of C sharp (TM), Java(TM.pdf;/home/hcngac/Zotero/storage/HUVFJSH7/1008.html},
  journal = {arXiv:1008.2748 [cs]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Programming Languages},
  primaryclass = {cs}
}

@inproceedings{jeongDistributedComputingFramework2017,
  title = {Towards a Distributed Computing Framework for {{Fog}}},
  booktitle = {2017 {{IEEE Fog World Congress}} ({{FWC}})},
  author = {Jeong, Taeyeol and Chung, Jaeyoon and Hong, James Won-Ki and Ha, Sangtae},
  year = {2017},
  month = oct,
  pages = {1--6},
  doi = {10.1109/FWC.2017.8368528},
  abstract = {Fog computing paradigm has introduced the concept of processing data near the data source. Unlike the cloud, fog computing includes devices with highly varying resources such as heterogeneous computing power, battery, bandwidth, delay, and mobility. The existing distributed computing frameworks, however, have mainly focused on the cloud environment where resources are highly consolidated and stable. This paper presents Crystal, a distributed computing framework for fog. An application consisting of one or multiple Crystal instances offers distributed processing and computing while taking advantage of location transparency, self-healing, auto-scaling and mobility support. Our prototype implementation of MapReduce on Crystal shows benefits of fog computing - fault-tolerant distributed processing over heterogeneous, unreliable, fog nodes while reducing overall latency, thanks to the framework enabling processing close to the data source.},
  file = {/home/hcngac/Zotero/storage/IJPGJ6LX/Jeong et al. - 2017 - Towards a distributed computing framework for Fog.pdf;/home/hcngac/Zotero/storage/I4PIZAZ6/8368528.html},
  keywords = {auto-scaling,cloud computing,Cloud computing,cloud environment,Crystal,Crystals,data handling,distributed computing framework,Distributed databases,Edge computing,fault tolerant computing,fault-tolerant distributed processing,fog computing paradigm,heterogeneous computing power,heterogeneous fog nodes,location transparency,MapReduce,mobile computing,mobility support,parallel processing,Peer-to-peer computing,Programming,self-healing}
}

@article{jonasCloudProgrammingSimplified2019,
  title = {Cloud {{Programming Simplified}}: {{A Berkeley View}} on {{Serverless Computing}}},
  shorttitle = {Cloud {{Programming Simplified}}},
  author = {Jonas, Eric and {Schleier-Smith}, Johann and Sreekanti, Vikram and Tsai, Chia-Che and Khandelwal, Anurag and Pu, Qifan and Shankar, Vaishaal and Carreira, Joao and Krauth, Karl and Yadwadkar, Neeraja and Gonzalez, Joseph E. and Popa, Raluca Ada and Stoica, Ion and Patterson, David A.},
  year = {2019},
  month = feb,
  abstract = {Serverless cloud computing handles virtually all the system administration operations needed to make it easier for programmers to use the cloud. It provides an interface that greatly simplifies cloud programming, and represents an evolution that parallels the transition from assembly language to high-level programming languages. This paper gives a quick history of cloud computing, including an accounting of the predictions of the 2009 Berkeley View of Cloud Computing paper, explains the motivation for serverless computing, describes applications that stretch the current limits of serverless, and then lists obstacles and research opportunities required for serverless computing to fulfill its full potential. Just as the 2009 paper identified challenges for the cloud and predicted they would be addressed and that cloud use would accelerate, we predict these issues are solvable and that serverless computing will grow to dominate the future of cloud computing.},
  archiveprefix = {arXiv},
  eprint = {1902.03383},
  eprinttype = {arxiv},
  file = {/home/hcngac/Zotero/storage/HRRJR5QL/Jonas et al. - 2019 - Cloud Programming Simplified A Berkeley View on S.pdf;/home/hcngac/Zotero/storage/L55JF4S5/1902.html},
  journal = {arXiv:1902.03383 [cs]},
  keywords = {Computer Science - Operating Systems,Favourite},
  primaryclass = {cs}
}

@inproceedings{kaffesCentralizedCoregranularScheduling2019,
  title = {Centralized {{Core}}-Granular {{Scheduling}} for {{Serverless Functions}}},
  booktitle = {Proceedings of the {{ACM Symposium}} on {{Cloud Computing}}},
  author = {Kaffes, Kostis and Yadwadkar, Neeraja J. and Kozyrakis, Christos},
  year = {2019},
  month = nov,
  pages = {158--164},
  publisher = {{Association for Computing Machinery}},
  address = {{Santa Cruz, CA, USA}},
  doi = {10.1145/3357223.3362709},
  abstract = {In recent years, many applications have started using serverless computing platforms primarily due to the ease of deployment and cost efficiency they offer. However, the existing scheduling mechanisms of serverless platforms fall short in catering to the unique characteristics of such applications: burstiness, short and variable execution times, statelessness and use of a single core. Specifically, the existing mechanisms fall short in meeting the requirements generated due to the combined effect of these characteristics: scheduling at a scale of millions of function invocations per second while achieving predictable performance. In this paper, we argue for a cluster-level centralized and core-granular scheduler for serverless functions. By maintaining a global view of the cluster resources, the centralized approach eliminates queue imbalances while the core granularity reduces interference; together these properties enable reduced performance variability. We expect such a scheduler to increase the adoption of serverless computing platforms by various latency and throughput sensitive applications.},
  file = {/home/hcngac/Zotero/storage/JR8559SH/Kaffes et al. - 2019 - Centralized Core-granular Scheduling for Serverles.pdf},
  isbn = {978-1-4503-6973-2},
  keywords = {cloud computing,Favourite,resource allocation,scheduling,serverless computing},
  series = {{{SoCC}} '19}
}

@misc{KeepingFunctionsWarm,
  title = {Keeping {{Functions Warm}} - {{How To Fix AWS Lambda Cold Start Issues}}},
  abstract = {Learn how to prevent cold start in your Lambda functions with the Serverless WarmUp plugin.},
  file = {/home/hcngac/Zotero/storage/3K9F8QMV/keep-your-lambdas-warm.html},
  howpublished = {https://serverless.com/blog/keep-your-lambdas-warm},
  journal = {serverless},
  keywords = {Cold Start},
  language = {en}
}

@inproceedings{kempeGossipbasedComputationAggregate2003,
  title = {Gossip-Based Computation of Aggregate Information},
  booktitle = {44th {{Annual IEEE Symposium}} on {{Foundations}} of {{Computer Science}}, 2003. {{Proceedings}}.},
  author = {Kempe, D. and Dobra, A. and Gehrke, J.},
  year = {2003},
  month = oct,
  pages = {482--491},
  issn = {0272-5428},
  doi = {10.1109/SFCS.2003.1238221},
  abstract = {Over the last decade, we have seen a revolution in connectivity between computers, and a resulting paradigm shift from centralized to highly distributed systems. With massive scale also comes massive instability, as node and link failures become the norm rather than the exception. For such highly volatile systems, decentralized gossip-based protocols are emerging as an approach to maintaining simplicity and scalability while achieving fault-tolerant information dissemination. In this paper, we study the problem of computing aggregates with gossip-style protocols. Our first contribution is an analysis of simple gossip-based protocols for the computation of sums, averages, random samples, quantiles, and other aggregate functions, and we show that our protocols converge exponentially fast to the true answer when using uniform gossip. Our second contribution is the definition of a precise notion of the speed with which a node's data diffuses through the network. We show that this diffusion speed is at the heart of the approximation guarantees for all of the above problems. We analyze the diffusion speed of uniform gossip in the presence of node and link failures, as well as for flooding-based mechanisms. The latter expose interesting connections to random walks on graphs.},
  file = {/home/hcngac/Zotero/storage/P3SM4I84/Kempe et al. - 2003 - Gossip-based computation of aggregate information.pdf;/home/hcngac/Zotero/storage/UHEW4YEA/1238221.html},
  keywords = {aggregate computation,aggregate functions,aggregate information,Aggregates,computer connectivity,computer network reliability,computer networks,Computer science,data diffusion,decentralized gossip-based protocols,diffusion speed,Distributed computing,distributed systems,exponential convergence,fault tolerant computing,Fault tolerant systems,fault-tolerant information dissemination,Favourite,flooding-based mechanisms,gossip-based computation,gossip-style protocols,instability,Large-scale systems,link failures,node failures,Peer to peer computing,protocols,Protocols,quantiles,random samples,random walks,Read,scalability,Scalability,Stress,Temperature sensors,uniform gossip,volatile systems}
}

@article{klimovicPocketElasticEphemeral,
  title = {Pocket: {{Elastic Ephemeral Storage}} for {{Serverless Analytics}}},
  author = {Klimovic, Ana and Trivedi, Animesh and Wang, Yawen and Pfefferle, Jonas and Stuedi, Patrick and Kozyrakis, Christos},
  pages = {19},
  abstract = {Serverless computing is becoming increasingly popular, enabling users to quickly launch thousands of shortlived tasks in the cloud with high elasticity and finegrain billing. These properties make serverless computing appealing for interactive data analytics. However exchanging intermediate data between execution stages in an analytics job is a key challenge as direct communication between serverless tasks is difficult. The natural approach is to store such ephemeral data in a remote data store. However, existing storage systems are not designed to meet the demands of serverless applications in terms of elasticity, performance, and cost. We present Pocket, an elastic, distributed data store that automatically scales to provide applications with desired performance at low cost. Pocket dynamically rightsizes resources across multiple dimensions (CPU cores, network bandwidth, storage capacity) and leverages multiple storage technologies to minimize cost while ensuring applications are not bottlenecked on I/O. We show that Pocket achieves similar performance to ElastiCache Redis for serverless analytics applications while reducing cost by almost 60\%.},
  file = {/home/hcngac/Zotero/storage/YZPIL9RJ/Klimovic et al. - Pocket Elastic Ephemeral Storage for Serverless A.pdf},
  keywords = {Favourite,Read,Storage},
  language = {en}
}

@misc{Knative,
  title = {Knative},
  abstract = {Knative provides a set of components for building modern, source-centric, and container-based applications that can run anywhere.},
  file = {/home/hcngac/Zotero/storage/STNQCB9S/knative.dev.html},
  howpublished = {https://knative.dev/},
  journal = {Knative},
  keywords = {Favourite,Infrastructure,Read},
  language = {en}
}

@misc{KnixmicrofunctionsKnix2020,
  title = {Knix-Microfunctions/Knix},
  year = {2020},
  month = jun,
  abstract = {KNIX MicroFunctions is a serverless computing platform that combines container-based resource isolation with a lightweight execution model using processes to significantly improve resource efficien...},
  copyright = {Apache-2.0},
  howpublished = {knix-microfunctions},
  keywords = {faas-platform,Favourite,function-as-a-service,Infrastructure,knative,kubernetes,Read,serverless,serverless-computing,serverless-framework}
}

@article{kuhnReusableCoordinationComponents2016,
  title = {Reusable {{Coordination Components}}: {{Reliable Development}} of {{Cooperative Information Systems}}},
  shorttitle = {Reusable {{Coordination Components}}},
  author = {K{\"u}hn, Eva},
  year = {2016},
  month = dec,
  volume = {25},
  pages = {1740001},
  publisher = {{World Scientific Publishing Co.}},
  issn = {0218-8430},
  doi = {10.1142/S0218843017400019},
  abstract = {Today's emerging trends like factory of the future, big data, Internet-of-things, intelligent traffic solutions, cyber-physical systems, wireless sensor networks, smart home, smart city and smart grid raise new challenges on software development. They are characterized by high concurrency, distribution and dynamics as well as huge numbers of heterogeneous devices, resources and users that must collaborate in a reliable way. The management of the interactions and dependencies between the participants is a complex task posing massive coordination problems. The here proposed approach is twofold: (i) to analyze similarities in the communication and synchronization behavior of such applications and to identify coordination patterns; and (ii) to give a precise specification of them by means of a suitable coordination model which enables the development of coordination pattern-based software components as solutions. The vision is to compose advanced cooperative information systems from proven, configurable, reusable, generic components that run on a suitable target platform, in order to reduce software development time, risks and costs. In this paper we delimit the idea of ``coordination patterns'' from other related pattern approaches and motivate the need for a well-defined model to specify them. Several coordination models to achieve this goal are discussed, and the advantages of a new coordination model termed the ``Peer Model'' are pointed out. The feasibility of the approach to identify coordination patterns, to model them and to provide generic components that can be reused in different scenarios through configuration and composition is evaluated by means of a coordination pattern found in several industrial use cases.},
  file = {/home/hcngac/Zotero/storage/JJJ8X967/Khn - 2016 - Reusable Coordination Components Reliable Develop.pdf;/home/hcngac/Zotero/storage/3Q68NRAC/S0218843017400019.html},
  journal = {International Journal of Cooperative Information Systems},
  number = {04}
}

@article{linMitigatingColdStarts2019,
  title = {Mitigating {{Cold Starts}} in {{Serverless Platforms}}: {{A Pool}}-{{Based Approach}}},
  shorttitle = {Mitigating {{Cold Starts}} in {{Serverless Platforms}}},
  author = {Lin, Ping-Min and Glikson, Alex},
  year = {2019},
  month = mar,
  abstract = {Rapid adoption of the 'serverless' (or Function-as-a-Service, FaaS) paradigm [8], pioneered by Amazon with AWS Lambda and followed by numerous commercial offerings and open source projects, introduces new challenges in designing the cloud infrastructure, balancing between performance and cost. While instant per-request elasticity that FaaS platforms typically offer application developers makes it possible to achieve high performance of bursty workloads without over-provisioning, such elasticity often involves extra latency associated with on-demand provisioning of individual runtime containers that serve the functions. This phenomenon is often called 'cold starts' [12], as opposed to the situation when a function is served by a pre-provisioned 'warm' container, ready to serve requests with close to zero overhead. Providers are constantly working on techniques aimed at reducing cold starts. A common approach to reduce cold starts is to maintain a pool of 'warm' containers, in anticipation of future requests. In this project, we address the cold start problem in serverless architectures, specifically under the Knative Serving FaaS platform. We implemented a pool of function instances and evaluated the latency compared with the original implementation, which resulted in an 85\% reduction of P99 response time for a single instance pool.},
  archiveprefix = {arXiv},
  eprint = {1903.12221},
  eprinttype = {arxiv},
  file = {/home/hcngac/Zotero/storage/FZCCBTYU/Lin and Glikson - 2019 - Mitigating Cold Starts in Serverless Platforms A .pdf},
  journal = {arXiv:1903.12221 [cs]},
  keywords = {Cold Start,Computer Science - Distributed; Parallel; and Cluster Computing},
  language = {en},
  primaryclass = {cs}
}

@misc{LoquatFrameworkLargescale,
  title = {Loquat: {{A}} Framework for Large-Scale Actor Communication on Edge Networks - {{IEEE Conference Publication}}},
  file = {/home/hcngac/Zotero/storage/LLW63DHF/Loquat A framework for large-scale actor communic.pdf;/home/hcngac/Zotero/storage/EUUC6TH3/7917624.html},
  howpublished = {https://ieeexplore.ieee.org/abstract/document/7917624}
}

@inproceedings{mancoMyVMLighter2017,
  title = {My {{VM}} Is {{Lighter}} (and {{Safer}}) than Your {{Container}}},
  booktitle = {Proceedings of the 26th {{Symposium}} on {{Operating Systems Principles}}  - {{SOSP}} '17},
  author = {Manco, Filipe and Lupu, Costin and Schmidt, Florian and Mendes, Jose and Kuenzer, Simon and Sati, Sumit and Yasukata, Kenichi and Raiciu, Costin and Huici, Felipe},
  year = {2017},
  pages = {218--233},
  publisher = {{ACM Press}},
  address = {{Shanghai, China}},
  doi = {10.1145/3132747.3132763},
  abstract = {Containers are in great demand because they are lightweight when compared to virtual machines. On the downside, containers o\dbend er weaker isolation than VMs, to the point where people run containers in virtual machines to achieve proper isolation. In this paper, we examine whether there is indeed a strict tradeo\dbend{} between isolation (VMs) and e\dbend ciency (containers). We \dbend nd that VMs can be as nimble as containers, as long as they are small and the toolstack is fast enough. We achieve lightweight VMs by using unikernels for specialized applications and with Tinyx, a tool that enables creating tailor-made, trimmed-down Linux virtual machines. By themselves, lightweight virtual machines are not enough to ensure good performance since the virtualization control plane (the toolstack) becomes the performance bottleneck. We present LightVM, a new virtualization solution based on Xen that is optimized to o\dbend er fast boot-times regardless of the number of active VMs. LightVM features a complete redesign of Xen's control plane, transforming its centralized operation to a distributed one where interactions with the hypervisor are reduced to a minimum. LightVM can boot a VM in 2.3ms, comparable to fork/exec on Linux (1ms), and two orders of magnitude faster than Docker. LightVM can pack thousands of LightVM guests on modest hardware with memory and CPU usage comparable to that of processes.},
  file = {/home/hcngac/Zotero/storage/5UDM5C62/Manco et al. - 2017 - My VM is Lighter (and Safer) than your Container.pdf;/home/hcngac/Zotero/storage/MQ2A9CNL/Manco et al. - 2017 - My VM is Lighter (and Safer) than your Container.pdf},
  isbn = {978-1-4503-5085-3},
  keywords = {Cold Start,containers,Favourite,hypervisor,Isolation,operating systems,Read,specialization,unikernels,virtual machine,Virtualization,Xen},
  language = {en}
}

@inproceedings{mogkFaulttolerantDistributedReactive2018,
  title = {Fault-Tolerant {{Distributed Reactive Programming}}},
  booktitle = {32nd {{European Conference}} on {{Object}}-{{Oriented Programming}} ({{ECOOP}} 2018)},
  author = {Mogk, Ragnar and Baumg{\"a}rtner, Lars and Salvaneschi, Guido and Freisleben, Bernd and Mezini, Mira},
  editor = {Millstein, Todd},
  year = {2018},
  volume = {109},
  pages = {1:1--1:26},
  publisher = {{Schloss Dagstuhl\textendash Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  issn = {1868-8969},
  doi = {10.4230/LIPIcs.ECOOP.2018.1},
  file = {/home/hcngac/Zotero/storage/SJJSN43M/Mogk et al. - 2018 - Fault-tolerant Distributed Reactive Programming.pdf;/home/hcngac/Zotero/storage/BYCXGUUN/9206.html},
  isbn = {978-3-95977-079-8},
  keywords = {CRDTs,distributed systems,error handling,fault tolerance,reactive programming,restoration,snapshots},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})}
}

@inproceedings{mohanAgileColdStarts2019,
  title = {Agile {{Cold Starts}} for {{Scalable Serverless}}},
  booktitle = {11th \{\vphantom\}{{USENIX}}\vphantom\{\} {{Workshop}} on {{Hot Topics}} in {{Cloud Computing}} ({{HotCloud}} 19)},
  author = {Mohan, Anup and Sane, Harshad and Doshi, Kshitij and Edupuganti, Saikrishna and Nayak, Naren and Sukhomlinov, Vadim},
  year = {2019},
  file = {/home/hcngac/Zotero/storage/36NFB7N7/Mohan et al. - 2019 - Agile Cold Starts for Scalable Serverless.pdf;/home/hcngac/Zotero/storage/2X6XA329/mohan.html},
  keywords = {Cold Start,Favourite,Read},
  language = {en}
}

@misc{NewAWSLambda2019,
  title = {New for {{AWS Lambda}} \textendash{} {{Predictable}} Start-up Times with {{Provisioned Concurrency}}},
  year = {2019},
  month = dec,
  abstract = {Since the launch of AWS Lambda five years ago, thousands of customers such as iRobot, Fender, and Expedia have experienced the benefits of the serverless operational model. Being able to spend less time on managing scaling and availability, builders are increasingly using serverless for more sophisticated workloads with more exacting latency requirements. As customers have [\ldots ]},
  chapter = {AWS Lambda},
  howpublished = {https://aws.amazon.com/blogs/compute/new-for-aws-lambda-predictable-start-up-times-with-provisioned-concurrency/},
  journal = {Amazon Web Services},
  keywords = {Cold Start},
  language = {en-US}
}

@inproceedings{newellOptimizingDistributedActor2016,
  title = {Optimizing Distributed Actor Systems for Dynamic Interactive Services},
  booktitle = {Proceedings of the {{Eleventh European Conference}} on {{Computer Systems}}},
  author = {Newell, Andrew and Kliot, Gabriel and Menache, Ishai and Gopalan, Aditya and Akiyama, Soramichi and Silberstein, Mark},
  year = {2016},
  month = apr,
  pages = {1--15},
  publisher = {{Association for Computing Machinery}},
  address = {{London, United Kingdom}},
  doi = {10.1145/2901318.2901343},
  abstract = {Distributed actor systems are widely used for developing interactive scalable cloud services, such as social networks and on-line games. By modeling an application as a dynamic set of lightweight communicating "actors", developers can easily build complex distributed applications, while the underlying runtime system deals with low-level complexities of a distributed environment. We present ActOp---a data-driven, application-independent runtime mechanism for optimizing end-to-end service latency of actor-based distributed applications. ActOp targets the two dominant factors affecting latency: the overhead of remote inter-actor communications across servers, and the intra-server queuing delay. ActOp automatically identifies frequently communicating actors and migrates them to the same server transparently to the running application. The migration decisions are driven by a novel scalable distributed graph partitioning algorithm which does not rely on a single server to store the whole communication graph, thereby enabling efficient actor placement even for applications with rapidly changing graphs (e.g., chat services). Further, each server autonomously reduces the queuing delay by learning an internal queuing model and configuring threads according to instantaneous request rate and application demands. We prototype ActOp by integrating it with Orleans -- a popular open-source actor system [4, 13]. Experiments with realistic workloads show latency improvements of up to 75\% for the 99th percentile, up to 63\% for the mean, with up to 2x increase in peak system throughput.},
  file = {/home/hcngac/Zotero/storage/H9IR46DA/Newell et al. - 2016 - Optimizing distributed actor systems for dynamic i.pdf},
  isbn = {978-1-4503-4240-7},
  series = {{{EuroSys}} '16}
}

@misc{OpensourcingGVisorSandboxed,
  title = {Open-Sourcing {{gVisor}}, a Sandboxed Container Runtime},
  abstract = {Containers have revolutionized how we develop, package, and deploy applications. However, the system surface exposed to containers is broad enough that man},
  file = {/home/hcngac/Zotero/storage/45A7FHQN/open-sourcing-gvisor-a-sandboxed-container-runtime.html},
  howpublished = {https://cloud.google.com/blog/products/gcp/open-sourcing-gvisor-a-sandboxed-container-runtime/},
  journal = {Google Cloud Blog},
  keywords = {Favourite,Read},
  language = {en}
}

@misc{OptimizingDistributedActor,
  title = {Optimizing Distributed Actor Systems for Dynamic Interactive Services | {{Proceedings}} of the {{Eleventh European Conference}} on {{Computer Systems}}},
  file = {/home/hcngac/Zotero/storage/ZUWQHWLT/Optimizing distributed actor systems for dynamic i.pdf;/home/hcngac/Zotero/storage/3B2JSFLX/2901318.html},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/2901318.2901343},
  language = {EN}
}

@article{schleier-smithServerlessFoundationsElastic,
  title = {Serverless {{Foundations}} for {{Elastic Database Systems}}},
  author = {{Schleier-Smith}, Johann},
  pages = {1},
  file = {/home/hcngac/Zotero/storage/ZQSC8EV8/Schleier-Smith - Serverless Foundations for Elastic Database System.pdf},
  keywords = {Favourite,Related},
  language = {en}
}

@article{shankarNumpywrenServerlessLinear2018,
  title = {Numpywren: Serverless Linear Algebra},
  shorttitle = {Numpywren},
  author = {Shankar, Vaishaal and Krauth, Karl and Pu, Qifan and Jonas, Eric and Venkataraman, Shivaram and Stoica, Ion and Recht, Benjamin and {Ragan-Kelley}, Jonathan},
  year = {2018},
  month = oct,
  abstract = {Linear algebra operations are widely used in scientific computing and machine learning applications. However, it is challenging for scientists and data analysts to run linear algebra at scales beyond a single machine. Traditional approaches either require access to supercomputing clusters, or impose configuration and cluster management challenges. In this paper we show how the disaggregation of storage and compute resources in so-called "serverless" environments, combined with compute-intensive workload characteristics, can be exploited to achieve elastic scalability and ease of management.   We present numpywren, a system for linear algebra built on a serverless architecture. We also introduce LAmbdaPACK, a domain-specific language designed to implement highly parallel linear algebra algorithms in a serverless setting. We show that, for certain linear algebra algorithms such as matrix multiply, singular value decomposition, and Cholesky decomposition, numpywren's performance (completion time) is within 33\% of ScaLAPACK, and its compute efficiency (total CPU-hours) is up to 240\% better due to elasticity, while providing an easier to use interface and better fault tolerance. At the same time, we show that the inability of serverless runtimes to exploit locality across the cores in a machine fundamentally limits their network efficiency, which limits performance on other algorithms such as QR factorization. This highlights how cloud providers could better support these types of computations through small changes in their infrastructure.},
  file = {/home/hcngac/Zotero/storage/IC8GZLY5/Shankar et al. - 2018 - numpywren serverless linear algebra.pdf;/home/hcngac/Zotero/storage/UDAYQVSC/1810.html},
  language = {en}
}

@inproceedings{shapiroConflictFreeReplicatedData2011,
  title = {Conflict-{{Free Replicated Data Types}}},
  booktitle = {Stabilization, {{Safety}}, and {{Security}} of {{Distributed Systems}}},
  author = {Shapiro, Marc and Pregui{\c c}a, Nuno and Baquero, Carlos and Zawirski, Marek},
  editor = {D{\'e}fago, Xavier and Petit, Franck and Villain, Vincent},
  year = {2011},
  pages = {386--400},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-24550-3_29},
  abstract = {Replicating data under Eventual Consistency (EC) allows any replica to accept updates without remote synchronisation. This ensures performance and scalability in large-scale distributed systems (e.g., clouds). However, published EC approaches are ad-hoc and error-prone. Under a formal Strong Eventual Consistency (SEC) model, we study sufficient conditions for convergence. A data type that satisfies these conditions is called a Conflict-free Replicated Data Type (CRDT). Replicas of any CRDT are guaranteed to converge in a self-stabilising manner, despite any number of failures. This paper formalises two popular approaches (state- and operation-based) and their relevant sufficient conditions. We study a number of useful CRDTs, such as sets with clean semantics, supporting both add and remove operations, and consider in depth the more complex Graph data type. CRDT types can be composed to develop large-scale distributed applications, and have interesting theoretical properties.},
  file = {/home/hcngac/Zotero/storage/ZASUZWAE/Shapiro et al. - 2011 - Conflict-Free Replicated Data Types.pdf},
  isbn = {978-3-642-24550-3},
  keywords = {Eventual Consistency,Favourite,Large-Scale Distributed Systems,Read,Replicated Shared Objects},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{shenBeamEndingMonolithic2016,
  title = {Beam: {{Ending Monolithic Applications}} for {{Connected Devices}}},
  shorttitle = {Beam},
  booktitle = {2016 \{\vphantom\}{{USENIX}}\vphantom\{\} {{Annual Technical Conference}} (\{\vphantom\}{{USENIX}}\vphantom\{\} \{\vphantom\}{{ATC}}\vphantom\{\} 16)},
  author = {Shen, Chenguang and Singh, Rayman Preet and Phanishayee, Amar and Kansal, Aman and Mahajan, Ratul},
  year = {2016},
  pages = {143--157},
  file = {/home/hcngac/Zotero/storage/73TVRJ9H/Shen et al. - 2016 - Beam Ending Monolithic Applications for Connected.pdf;/home/hcngac/Zotero/storage/MHLIQ8GT/Shen et al. - 2016 - Beam Ending Monolithic Applications for Connected.pdf;/home/hcngac/Zotero/storage/KEP5S3BC/shen.html},
  isbn = {978-1-931971-30-0},
  language = {en}
}

@inproceedings{solaimanWLECNotCold2020,
  title = {{{WLEC}}: {{A Not So Cold Architecture}} to {{Mitigate Cold Start Problem}} in {{Serverless Computing}}},
  shorttitle = {{{WLEC}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Cloud Engineering}} ({{IC2E}})},
  author = {Solaiman, Khondokar and Adnan, Muhammad Abdullah},
  year = {2020},
  month = apr,
  pages = {144--153},
  doi = {10.1109/IC2E48712.2020.00022},
  abstract = {As serverless computing gains popularity among developers for its low costing and elasticity, it has emerged as a promising research field in computer science. Despite its popularity, the cold start remains an issue that needs more attention. In this paper, we address the cold start problem of the serverless platform. We propose WLEC, a container management architecture to minimize the cold start time. WLEC uses a modified S2LRU structure, called S2LRU ++ with an additional third queue. We implement WLEC in OpenLambda and evaluate it in both AWS and Local VM environment with six different metrics in addition to one real-time image resizing application. Among improvements in all metrics, 50\% less memory consumption compared to the all-warm method and 31\% average cold start duration reduction compared to the no-warm method are the most notable ones.},
  file = {/home/hcngac/Zotero/storage/EAIZT9VI/Solaiman and Adnan - 2020 - WLEC A Not So Cold Architecture to Mitigate Cold .pdf;/home/hcngac/Zotero/storage/7IUVQGKW/9096489.html},
  keywords = {additional third queue,cloud computing,Cloud computing,cold start,Cold Start,cold start duration reduction,cold start time,Computational modeling,Computer architecture,computer science,container,container management architecture,Containers,Favourite,lambda functions,modified S2LRU structure,queueing theory,Read,real-time image resizing application,Runtime,serverless computing,Servers,storage management,virtual machines,Virtualization,warm up queue,WLEC}
}

@article{sreekantiCloudburstStatefulFunctionsasaService2020,
  title = {Cloudburst: {{Stateful Functions}}-as-a-{{Service}}},
  shorttitle = {Cloudburst},
  author = {Sreekanti, Vikram and Wu, Chenggang and Lin, Xiayue Charles and {Schleier-Smith}, Johann and Faleiro, Jose M. and Gonzalez, Joseph E. and Hellerstein, Joseph M. and Tumanov, Alexey},
  year = {2020},
  month = jan,
  abstract = {Function-as-a-Service (FaaS) platforms and "serverless" cloud computing are becoming increasingly popular. Current FaaS offerings are targeted at stateless functions that do minimal I/O and communication. We argue that the benefits of serverless computing can be extended to a broader range of applications and algorithms. We present the design and implementation of Cloudburst, a stateful FaaS platform that provides familiar Python programming with low-latency mutable state and communication, while maintaining the autoscaling benefits of serverless computing. Cloudburst accomplishes this by leveraging Anna, an autoscaling key-value store, for state sharing and overlay routing combined with mutable caches co-located with function executors for data locality. Performant cache consistency emerges as a key challenge in this architecture. To this end, Cloudburst provides a combination of lattice-encapsulated state and new definitions and protocols for distributed session consistency. Empirical results on benchmarks and diverse applications show that Cloudburst makes stateful functions practical, reducing the state-management overheads of current FaaS platforms by orders of magnitude while also improving the state of the art in serverless consistency.},
  archiveprefix = {arXiv},
  eprint = {2001.04592},
  eprinttype = {arxiv},
  file = {/home/hcngac/Zotero/storage/7PFQK5YQ/Sreekanti et al. - 2020 - Cloudburst Stateful Functions-as-a-Service.pdf;/home/hcngac/Zotero/storage/M3RBKDPS/2001.html},
  journal = {arXiv:2001.04592 [cs]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Favourite},
  primaryclass = {cs}
}

@inproceedings{wangInfiniCacheExploitingEphemeral2020,
  title = {{{InfiniCache}}: {{Exploiting Ephemeral Serverless Functions}} to {{Build}} a {{Cost}}-{{Effective Memory Cache}}},
  shorttitle = {{{InfiniCache}}},
  booktitle = {18th \{\vphantom\}{{USENIX}}\vphantom\{\} {{Conference}} on {{File}} and {{Storage Technologies}} (\{\vphantom\}{{FAST}}\vphantom\{\} 20)},
  author = {Wang, Ao and Zhang, Jingyuan and Ma, Xiaolong and Anwar, Ali and Rupprecht, Lukas and Skourtis, Dimitrios and Tarasov, Vasily and Yan, Feng and Cheng, Yue},
  year = {2020},
  pages = {267--281},
  file = {/home/hcngac/Zotero/storage/XJ7NJCMZ/Wang et al. - 2020 - InfiniCache Exploiting Ephemeral Serverless Funct.pdf;/home/hcngac/Zotero/storage/SJ9T8YHJ/wang-ao.html},
  isbn = {978-1-939133-12-0},
  keywords = {Favourite,Storage},
  language = {en}
}

@inproceedings{wangPeekingCurtainsServerless2018,
  title = {Peeking {{Behind}} the {{Curtains}} of {{Serverless Platforms}}},
  booktitle = {2018 \{\vphantom\}{{USENIX}}\vphantom\{\} {{Annual Technical Conference}} (\{\vphantom\}{{USENIX}}\vphantom\{\} \{\vphantom\}{{ATC}}\vphantom\{\} 18)},
  author = {Wang, Liang and Li, Mengyuan and Zhang, Yinqian and Ristenpart, Thomas and Swift, Michael},
  year = {2018},
  pages = {133--146},
  file = {/home/hcngac/Zotero/storage/8DF86ZVR/Wang et al. - 2018 - Peeking Behind the Curtains of Serverless Platform.pdf;/home/hcngac/Zotero/storage/J3RQXKMJ/wang-liang.html},
  isbn = {978-1-939133-01-4},
  keywords = {Cold Start,Communication,Favourite,Infrastructure,Isolation,Storage},
  language = {en}
}

@misc{WritingCloudFunctions,
  title = {Writing {{Cloud Functions}} | {{Cloud Functions Documentation}}},
  file = {/home/hcngac/Zotero/storage/2I8VA3PU/writing.html},
  howpublished = {https://cloud.google.com/functions/docs/writing},
  journal = {Google Cloud},
  language = {en}
}

@article{wuAnnaKVSAny2019,
  title = {Anna: {{A KVS For Any Scale}}},
  shorttitle = {Anna},
  author = {Wu, Chenggang and Faleiro, Jose and Lin, Yihan and Hellerstein, Joseph},
  year = {2019},
  pages = {1--1},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2019.2898401},
  abstract = {Modern cloud providers offer dense hardware with multiple cores and large memories, hosted in global platforms. This raises the challenge of implementing high-performance software systems that can effectively scale from a single core to multicore to the globe. Conventional wisdom says that software designed for one scale point needs to be rewritten when scaling up by \$10 - 100\textbackslash times\$. In contrast, we explore how a system can be architected to scale across many orders of magnitude by design. We explore this challenge in the context of a new key-value store system called Anna: a partitioned, multi-mastered system that achieves high performance and elasticity via wait-free execution and coordination-free consistency. Our design rests on a simple architecture of coordination-free actors that perform state update via merge of lattice-based composite data structures. We demonstrate that a wide variety of consistency models can be elegantly implemented in this architecture with unprecedented consistency, smooth fine-grained elasticity, and performance that far exceeds the state of the art.},
  file = {/home/hcngac/Zotero/storage/DYQPY6XB/Wu et al. - 2019 - Anna A KVS For Any Scale.pdf;/home/hcngac/Zotero/storage/7ATPNZ8J/8640246.html},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  keywords = {Favourite,Hardware,Lattices,Message systems,Multicore processing,Programming,Scalability,Servers,Storage}
}

@article{wuAutoscalingTieredCloud2019,
  title = {Autoscaling Tiered Cloud Storage in {{Anna}}},
  author = {Wu, Chenggang and Sreekanti, Vikram and Hellerstein, Joseph M.},
  year = {2019},
  month = feb,
  volume = {12},
  pages = {624--638},
  issn = {2150-8097},
  doi = {10.14778/3311880.3311881},
  abstract = {In this paper, we describe how we extended a distributed key-value store called Anna into an autoscaling, multi-tier service for the cloud. In its extended form, Anna is designed to overcome the narrow cost-performance limitations typical of current cloud storage systems. We describe three key aspects of Anna's new design: multi-master selective replication of hot keys, a vertical tiering of storage layers with different cost-performance tradeoffs, and horizontal elasticity of each tier to add and remove nodes in response to load dynamics. Anna's policy engine uses these mechanisms to balance service-level objectives around cost, latency and fault tolerance. Experimental results explore the behavior of Anna's mechanisms and policy, exhibiting orders of magnitude efficiency improvements over both commodity cloud KVS services and research systems.},
  file = {/home/hcngac/Zotero/storage/JM3YWWKJ/Wu et al. - 2019 - Autoscaling tiered cloud storage in Anna.pdf},
  journal = {Proceedings of the VLDB Endowment},
  keywords = {Favourite,Read,Storage},
  number = {6}
}

@inproceedings{youngTrueCostContaining2019,
  title = {The {{True Cost}} of {{Containing}}: {{A gVisor Case Study}}},
  shorttitle = {The {{True Cost}} of {{Containing}}},
  booktitle = {11th \{\vphantom\}{{USENIX}}\vphantom\{\} {{Workshop}} on {{Hot Topics}} in {{Cloud Computing}} ({{HotCloud}} 19)},
  author = {Young, Ethan G. and Zhu, Pengfei and {Caraza-Harter}, Tyler and {Arpaci-Dusseau}, Andrea C. and {Arpaci-Dusseau}, Remzi H.},
  year = {2019},
  file = {/home/hcngac/Zotero/storage/VJ9NBJWR/Young et al. - 2019 - The True Cost of Containing A gVisor Case Study.pdf;/home/hcngac/Zotero/storage/FF5KTAYN/young.html},
  keywords = {Favourite,Read},
  language = {en}
}


